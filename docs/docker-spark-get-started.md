# Spocker - Multi-Host Dockerized Spark using Docker Swarm

Hi there, this a step-by-step guide to get a Dockerized Spark (StandAlone) up-and-running on a cluster using the Docker Swarm which is a multi-host container orchestartion tool.


## Requirements
- Multiple VMs (atleast 2) on the same network, with internet access on atleast one VM.
- Docker installed on each VM.

## Steps

1. Setup a Docker swarm on the connected VMs.
2. Deploy the dockerized Spark stack from the Docker Swarm Manager/Leader.
3. Submit a spark-job to the Spark cluster.


## 1. Setup Docker Swarm

## 2. Deploy Spark to Docker Swarm

## 3. Submit a Spark job